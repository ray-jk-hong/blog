<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>块设备基础 | ray.jk.hong</title><meta name="author" content="ray.jk.hong"><meta name="copyright" content="ray.jk.hong"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="数据处理流程  假设传文件fd和offset调用read()系统调用，会以如下数据流最终调用到块设备驱动。  vfs决定是从Page Cache(上图中的Disk cache)中读，还是继续从文件系统中读。Page Cache会在后续补充，这里只看从文件系统中读的场景。 vfs通过相应的（每个目录的文件系统不同）文件系统接口找到对应的inode。 Generic block layer准备组织数据">
<meta property="og:type" content="article">
<meta property="og:title" content="块设备基础">
<meta property="og:url" content="https://ray-jk-hong.github.io/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/index.html">
<meta property="og:site_name" content="ray.jk.hong">
<meta property="og:description" content="数据处理流程  假设传文件fd和offset调用read()系统调用，会以如下数据流最终调用到块设备驱动。  vfs决定是从Page Cache(上图中的Disk cache)中读，还是继续从文件系统中读。Page Cache会在后续补充，这里只看从文件系统中读的场景。 vfs通过相应的（每个目录的文件系统不同）文件系统接口找到对应的inode。 Generic block layer准备组织数据">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ray-jk-hong.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2025-08-13T10:47:43.530Z">
<meta property="article:modified_time" content="2025-08-13T10:47:43.531Z">
<meta property="article:author" content="ray.jk.hong">
<meta property="article:tag" content="文件系统">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ray-jk-hong.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "块设备基础",
  "url": "https://ray-jk-hong.github.io/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/",
  "image": "https://ray-jk-hong.github.io/img/butterfly-icon.png",
  "datePublished": "2025-08-13T10:47:43.530Z",
  "dateModified": "2025-08-13T10:47:43.531Z",
  "author": [
    {
      "@type": "Person",
      "name": "ray.jk.hong",
      "url": "https://ray-jk-hong.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/blog/img/favicon.png"><link rel="canonical" href="https://ray-jk-hong.github.io/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/blog/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/blog/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '块设备基础',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><script src="/js/bandev.js"></script><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/blog/"></a><a class="nav-page-title" href="/blog/"><span class="site-name">块设备基础</span></a></span><div id="menus"></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">块设备基础</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-categories"><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/blog/categories/linux/">linux</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><h2 id="数据处理流程"><a href="#数据处理流程" class="headerlink" title="数据处理流程"></a>数据处理流程</h2><img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87%E6%9E%B6%E6%9E%84.png" class="" title="图片描述">

<p>假设传文件fd和offset调用read()系统调用，会以如下数据流最终调用到块设备驱动。</p>
<ol>
<li>vfs决定是从Page Cache(上图中的Disk cache)中读，还是继续从文件系统中读。Page Cache会在后续补充，这里只看从文件系统中读的场景。</li>
<li>vfs通过相应的（每个目录的文件系统不同）文件系统接口找到对应的inode。</li>
<li>Generic block layer准备组织数据（IO调度）并将数据发给真正的块设备。数据会被组织成一连串bio。</li>
<li>最终，块设备驱动的request回调会被调用，并将数据真正写入到块设备中（SSD或者tmpfs等）。</li>
</ol>
<p>这篇主要讲3，4涉及的块设备。1，2的内容参考<a href="https://ray-jk-hong.github.io/blog/2025/04/17/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/">文件系统篇</a>。</p>
<h2 id="块设备驱动"><a href="#块设备驱动" class="headerlink" title="块设备驱动"></a>块设备驱动</h2><h3 id="块设备驱动介绍"><a href="#块设备驱动介绍" class="headerlink" title="块设备驱动介绍"></a>块设备驱动介绍</h3><p>我们先讲底层驱动，再讲其上层的块设备内容(block目录下的内容)。<br>块设备驱动，一句话就是接收下面的块设备发过来的数据，并将数据写入到真正的设备的过程（这里也可能是内存，例如shmem）。</p>
<h3 id="块设备驱动-1"><a href="#块设备驱动-1" class="headerlink" title="块设备驱动"></a>块设备驱动</h3><h4 id="注册块设备主设备号"><a href="#注册块设备主设备号" class="headerlink" title="注册块设备主设备号"></a>注册块设备主设备号</h4><p>下面两个函数是块设备的主设备申请和释放的接口：<br><code>int register_blkdev(unsigned int major, const char *name)</code><br><code>int unregister_blkdev(unsigned int major, const char *name)</code></p>
<p><code>register_blkdev</code>第一个参数可以输入0，返回值小于0表示是错误，不是的话就是返回的设备的主设备号。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> major = register_blkdev(<span class="number">0</span>, <span class="string">&quot;my_blkdev&quot;</span>);</span><br><span class="line"><span class="keyword">if</span> (major &lt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> major;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="创建gendisk"><a href="#创建gendisk" class="headerlink" title="创建gendisk"></a>创建<code>gendisk</code></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">blk_mq_tag_set</span> <span class="title">set</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">gendisk</span> *<span class="title">disk</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">lock_class_key</span> <span class="title">lkclass</span>;</span></span><br><span class="line"><span class="type">void</span> *queuedata;</span><br><span class="line"><span class="type">int</span> ret;</span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span>.ops = &amp;my_mq_ops;</span><br><span class="line"><span class="built_in">set</span>.queue_depth = <span class="number">1024</span>;</span><br><span class="line">...</span><br><span class="line">ret = blk_mq_alloc_tag_set(&amp;<span class="built_in">set</span>);</span><br><span class="line"><span class="keyword">if</span> (ret != <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">disk = __blk_mq_alloc_disk(&amp;<span class="built_in">set</span>, queuedata, &amp;lkclass);</span><br><span class="line"><span class="keyword">if</span> (disk == <span class="literal">NULL</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> -ENOMEM;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">disk-&gt;fops = &amp;my_blk_ops;</span><br><span class="line">ret = device_add_disk(&amp;my_dev-&gt;dev, disk, my_attr_groups);</span><br><span class="line"><span class="keyword">if</span> (ret != <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>按上述流程创建完gendisk之后，块设备已经都生成了。<br>创建的结构体以及结构体之间的关系如下：</p>
<img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87-01.png" class="" title="图片描述">

<p>上图中标红色的部分就是接收块设备（block）下发数据的接口。</p>
<p>注意：以前的<code>blk_init_queue</code>方式已经被废弃删除。</p>
<h4 id="块设备驱动数据流"><a href="#块设备驱动数据流" class="headerlink" title="块设备驱动数据流"></a>块设备驱动数据流</h4><p>以下简单描述一下在用户态mmap一个文件系统中的文件并写数据的时候的调用流程。图如下：</p>
<img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87%E6%95%B0%E6%8D%AE%E6%B5%81.png" class="" title="图片描述">

<ol>
<li>用户态mmap一个ext4类型分区的文件，此时会生成一个vm_struct并准备好vm_file文件指针，可以知道是哪个文件。</li>
<li>写入数据时，page会准备好标记位脏页。当回写的时候，通过page-&gt;mapping可以找到address_space_operation并调用到ext4_writepage。</li>
<li>ext4将要拷贝的数据组织成一个bio发送给Block模块中</li>
<li>Block模块申请request结构体，将bio经过调度之后加到request结构体中。request将被加到对应的块设备驱动的requst_queue中，并调用块设备驱动的request函数。</li>
<li>块设备驱动的request函数，就可以处理这些要拷贝的数据了。</li>
</ol>
<p>上面这个只是最简单的数据流的说明，其实还有很多细节和内容。</p>
<h2 id="块设备"><a href="#块设备" class="headerlink" title="块设备"></a>块设备</h2><p>我们以写ext4文件系统中的文件为例，看一下数据是以什么流程一步一步写到块设备驱动的（只看设置<code>O_DIRECT</code>直接方式，写入到page cache的流程看PageCache篇。下面按结构体转换的顺序看一下数据流是怎么样的。</p>
<h3 id="struct-kiocb-struct-iov-iter"><a href="#struct-kiocb-struct-iov-iter" class="headerlink" title="struct kiocb&#x2F;struct iov_iter"></a><code>struct kiocb</code>&#x2F;<code>struct iov_iter</code></h3><p>在<code>ext4</code>文件系统创建文件的时候、<code>struct file_operations ext4_file_operations</code>会被设置到<code>inode</code>。<br>在调用<code>write</code>系统调用过程中、<code>vfs_write</code>会将<code>buf</code>&#x2F;<code>offset</code>&#x2F;<code>len</code>等封装成<code>struct kiocb</code>&#x2F;<code>struct iov_iter</code>调用<code>ext4_file_write_iter</code>。流程如下：</p>
<img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87-03.png" class="" title="图片描述">

<p><code>struct kiocb</code>&#x2F;<code>struct iov_iter</code>结构体如下：</p>
<img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87-04.png" class="" title="图片描述">

<h3 id="struct-bio-struct-bio-vec"><a href="#struct-bio-struct-bio-vec" class="headerlink" title="struct bio&#x2F;struct bio_vec"></a><code>struct bio</code>&#x2F;<code>struct bio_vec</code></h3><p>总结起来、这一步就是将<code>struct kiocb</code>&#x2F;<code>struct iov_iter</code>两个结构体、转成<code>struct bio</code>&#x2F;<code>struct bio_vec</code>结构体并传给block层。</p>
<p>下面重点介绍一下<code>struct bio</code>&#x2F;<code>struct bio_vec</code>等几个关键结构体。<br>下面是几个结构体之间的关系：</p>
<img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87-07.png" class="" title="图片描述">

<ul>
<li><code>struct bio</code>：表示了一段连续的磁盘空间、bvec_iter.bi_sector和bvec_iter.bi_size分别记录这段连续物理磁盘的起始段号和总大小</li>
<li><code>struct bio_vec</code>：描述了一段连续的mem空间、大小不超过一个page<br>疑问：sys_write产生的bio只有一个bio_vec、但是sys_read产生的bio却有多个bio_vec、为什么？</li>
</ul>
<h3 id="iomap"><a href="#iomap" class="headerlink" title="iomap"></a>iomap</h3><p><code>ext4</code>调用<code>iomap</code>的流程如下：</p>
<img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87-05.png" class="" title="图片描述">

<p><code>filemap_write_and_wait_range</code>就是要将这次要写的范围内的<code>dirty</code>的数据先写回、避免数据错乱。</p>
<p>下面是将文件的<code>buf</code>&#x2F;<code>offset</code>&#x2F;<code>len</code>等转成<code>struct bio</code>的过程：</p>
<img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87-06.png" class="" title="图片描述">

<ol>
<li><code>struct ext4_map_blocks</code>：</li>
</ol>
<ul>
<li><code>m_pblk</code>：是指文件系统中的物理block、通过<code>ext4_es_pblock</code>计算。</li>
<li><code>m_lblk</code>：通过<code>offset &gt;&gt; blktis</code>计算的文件的偏移</li>
</ul>
<ol start="2">
<li><code>struct bio</code>：一个<code>struct bio</code>写数据的最小单位一个page或者多个page。<code>struct bio</code>有多个<code>struct bio_vec</code>。</li>
</ol>
<ul>
<li><code>struct bio_vec</code>：此结构体就是文件系统中对应的<code>segment</code>、此数据的大小不能大于一个page（看数据结构也能猜到、因为只有一个page指针）。其中page就是通过<code>struct iov_iter</code>-&gt;<code>struct iovec.iov_base</code>、使用<code>gup</code>(get user page)获取的的page地址。</li>
</ul>
<p>更为仔细的转换过程如下：</p>
<ol>
<li>如下图所示、读文件1024到5120字节的内容。上图中的<code>file</code>表示文件结构体。下面的mem表示<code>address_space</code>中的<code>page</code>、注意是已经申请完放到<code>address_space</code>中的。</li>
</ol>
<img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87-08.png" class="" title="图片描述">

<ol start="2">
<li>如下图所示、文件系统会通过调用<code>ext4_iomap_begin</code>函数找到在对应文件系统中占据4, 5, 6, 7四个sector、这时可以生成一个<code>bio</code>。</li>
</ol>
<img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87-09.png" class="" title="图片描述">

<p>查找文件系统中的sector的流程是：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filemap_get_pages -&gt; filemap_readahead -&gt; page_cache_async_ra -&gt; ondemand_readahead -&gt; do_page_cache_ra -&gt; page_cache_ra_unbounded -&gt; read_pages -&gt; aops.readahead -&gt; ext4_readahead -&gt; ext4_mpage_readpages -&gt; ext4_map_blocks -&gt; ext4_ext_map_blocks -&gt; ops.iomap_begin</span><br></pre></td></tr></table></figure>

<p>申请bio的流程如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iomap_readahead -&gt; iomap_readahead_iter -&gt; iomap_readpage_iter -&gt; bio_alloc/bio_add_folio</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>如下图所示、page中还剩一部分需要找到sector并生成一个新的bio</li>
</ol>
<img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87-10.png" class="" title="图片描述">

<p>这个流程与2.中描述的基本一致。</p>
<ol start="4">
<li>如下图所示、后续要写的2048字节在<code>address_space</code>中寻找、下面是已经找到的场景（如果没有的话需要重新申请page挂接到address_space）。</li>
</ol>
<img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87-11.png" class="" title="图片描述">

<p>读写的流程如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sys_read：ondemand_readahead -&gt; do_page_cache_ra -&gt; page_cache_ra_unbounded -&gt; xa_load（在sys_read流程中，因为一开始就会把所有的folio都拿到，不是一个一个拿的）</span><br><span class="line">iomap_readahead_iter -&gt; readahead_folio</span><br><span class="line">sys_write：</span><br><span class="line">xfs_file_write_iter -&gt; xfs_file_buffered_write -&gt; iomap_file_buffered_write -&gt; iomap_write_iter -&gt; iomap_write_begin -&gt; __filemap_get_folio -&gt; mapping_get_entry/filemap_add_folio （在sys_write流程，是用完一个folio，再申请新的folio）</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>如下图所示、从address_space中拿到的一个page中的一部分数据、已经有数据了。</li>
</ol>
<img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87-12.png" class="" title="图片描述">

<p>代码流程如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iomap_readahead_iter -&gt; iomap_adjust_read_range</span><br></pre></td></tr></table></figure>

<p>疑问：page数据是否是uptodate不是以一个page为粒度吗？？</p>
<ol start="6">
<li>如下图所示、绿色部分的数据在读文件系统之后发现其数据在14、15两个sector中。</li>
</ol>
<img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87-13.png" class="" title="图片描述">

<p>代码流程如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iomap_readahead -&gt; iomap_iter -&gt; ops.iomap_begin</span><br></pre></td></tr></table></figure>

<ol start="7">
<li>如下图所示、14&#x2F;15两个sector要重新申请一个bio。</li>
</ol>
<img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87-14.png" class="" title="图片描述">

<p>代码流程如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sys_read：iomap_readpage_iter -&gt; bio_add_folio -&gt; __bio_try_merge_page</span><br><span class="line">sys_write：xfs_file_write_iter -&gt; xfs_file_buffered_write -&gt; iomap_file_buffered_write -&gt; iomap_write_iter -&gt; iomap_write_begin -&gt; __iomap_write_begin -&gt; iomap_read_folio_sync -&gt; bio_init/bio_add_folio （一个bio只有一个bio_vec）</span><br></pre></td></tr></table></figure>

<ol start="8">
<li>完成全部的bio生成、可以调用submit_bio或者submit_bio_wait进入下一级的request流程。</li>
</ol>
<img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87-15.png" class="" title="图片描述">

<h3 id="request"><a href="#request" class="headerlink" title="request"></a>request</h3><p>bio打包完成，现在我们需要将它发送给磁盘。一个bio描述了一段连续的磁盘空间，如果两个bio在磁盘物理地址正好是相邻的，组合起来也刚好是一段连续的磁盘空间，对于这种情况实际上也只需要给磁盘发送一次请求就够了，不需要将两个bio分别单独发给磁盘。因此为了将bio重新封装，把相邻的bio进行合并，kernel又提出了新的结构体struct request。</p>
<p>request与bio之间的关系如下：</p>
<img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87-15.png" class="" title="图片描述">

<p>相邻的bio通过bio.bi_next构成一个链表，挂载到struct request上，由request进行统一管理，request.bio记录链表头，request.biotail记录链表尾。<br>为了使bio找到合适自己request，我们也需要将request串成链表统一管理，有新的bio来到时，只需要遍历链表就可以找到合适自己的request，进行merge。<br>大家可能会有一些疑问，为什么内核不将两个连续的bio合并成一个bio，而仅仅用一个request将两个连续的bio串起来管理呢？因为每个bio都有自己的上下文环境，在很多场景下（参考submit_bio_wait），进程需要等这个bio结束，才能继续进行下一步操作，如果bio被合并没了，那么这个bio是否执行完成也无法通知到自己的上下文。</p>
<p>bio, bio_vec, request几个结构体在虚拟地址和磁盘上的连续性如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>磁盘物理地址</th>
<th>用户文件地址</th>
<th>内存物理地址</th>
</tr>
</thead>
<tbody><tr>
<td>bio_vec</td>
<td>地址连续</td>
<td>地址连续</td>
<td>地址连续</td>
</tr>
<tr>
<td>bio</td>
<td>地址连续</td>
<td>地址连续</td>
<td>地址不连续</td>
</tr>
<tr>
<td>request</td>
<td>地址连续</td>
<td>地址不连续</td>
<td>地址不连续</td>
</tr>
</tbody></table>
<h3 id="current-plug链表"><a href="#current-plug链表" class="headerlink" title="current-&gt;plug链表"></a>current-&gt;plug链表</h3><p>为了更好的管理<code>request</code>，内核定义了一个per task的结构体<code>struct blk_plug</code>，我们可以在<code>struct task_struct</code>找到它。<br>同一个进程的request都会暂时挂载到<code>blk_plug.mq_list</code>中, 新的bio到来时，会遍历<code>blk_plug.mq_list</code>如果发现存在合适的request，那么就不必再申请新的<code>request</code>了，只需要在已经存在的<code>request.bio</code>链表上新增成员就可以了，具体是放在链表头还是链表尾取决于磁盘的相对位置（参考函数blk_attempt_bio_merge）。</p>
<p>代码流程：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">代码参考：</span><br><span class="line">链表新增节点位置：submit_bio -&gt; submit_bio_noacct -&gt; submit_bio_noacct_nocheck -&gt; __ submit_bio_noacct_mq/ __ submit_bio_noacct -&gt; __ submit_bio -&gt; blk_mq_submit_bio -&gt; blk_add_rq_to_plug</span><br><span class="line">链表遍历位置：__ submit_bio -&gt; blk_mq_submit_bio -&gt; blk_mq_get_cached_request -&gt; blk_mq_attempt_bio_merge -&gt; blk_attempt_plug_merge</span><br></pre></td></tr></table></figure>

<p>值得注意的是在2013年之后的版本中，plug机制已经不能满足硬件需求了，kernel又提供了新的机制来替代它，所以当前版本current-&gt;plug并不是必须的，例如sys_read中使用了plug机制，但是sys_write已经不再使用plug机制。具体是否使用取决于代码作者是否在调用submit_bio函数前后调用了blk_start_plug和blk_finish_plug两个函数对blk_plug进行初始化。</p>
<p>这里说一句题外话，当bio生成后必须调用submit_bio函数，将bio再次封装后发送给disk。disk本质上属于一个块设备，使用“md”（例如软件 RAID）和“dm”（例如 LVM2）之类的虚拟块设备很可能会产生一个块设备堆，每个块设备都会修改一个 bio并将其发送到堆栈中的下一个块设备，大量块设备会造成内核调用堆栈溢出。为了避免submit_bio函数嵌套导致的内核堆栈溢出，kernel会将同一进程的bio统一放到current-&gt;bio_list暂时存储，submit bio时从bio_list中一个一个取出进行submit。</p>
<p>代码流程：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">代码参考：submit_bio -&gt; submit_bio_noacct -&gt; submit_bio_noacct_nocheck -&gt; bio_list_add</span><br></pre></td></tr></table></figure>

<h3 id="multi-queue多队列排队机制"><a href="#multi-queue多队列排队机制" class="headerlink" title="multi-queue多队列排队机制"></a>multi-queue多队列排队机制</h3><p>2013年之后引入新的patch，新增了io多队列排队机制。之前的plug仅仅在per task维度进行管理，显然对于日益复杂的硬件来说是远远不够的。<br>现在一个物理机常常会接多块磁盘，每个磁盘可能归属不同厂商，硬件配置与软件驱动都不相同。所以kernel在per disk的维度为每个磁盘构建了<code>blk_mq_ctx</code>（软件队列）和<code>blk_mq_hw_ctx</code>（硬件队列）来管理本磁盘所有的request，他们之间的关系如图所示</p>
<p>request一般不需要往<code>bk_mq_hw_ctx.dispatch</code>放，它仅仅用作内核或设备资源不足时（非错误），由函数<code>blk_mq_request_bypass_insert</code>将request暂时存储到这个链表，下次flush时重试，</p>
<p>前面提到过disk本质上属于一个块设备，如果对kernel设备驱动框架了解的同学可以明白，在<code>scsi adapter driver</code>代码的probe函数中会扫描所有的scsi devices（对于scsi协议的disk而言，disk即是一个scsi device），如果scsi找到了存在的disk设备，那么便会根据具体硬件对disk相关数据结构进行初始化，其中就包括<code>struct blk_mq_ctx</code>和<code>blk_mq_hw_ctx</code>，所以也进一步说明ctx和hctx是per disk的。</p>
<p>ctx申请代码参考：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__scsi_scan_target -&gt; scsi_report_lun_scan -&gt; scsi_alloc_sdev -&gt; blk_mq_init_queue -&gt; blk_mq_init_queue_data -&gt; blk_mq_init_allocated_queue -&gt; blk_mq_alloc_ctxs/blk_mq_realloc_hw_ctxs</span><br></pre></td></tr></table></figure>

<h4 id="blk-mq-ctx软件队列"><a href="#blk-mq-ctx软件队列" class="headerlink" title="blk_mq_ctx软件队列"></a>blk_mq_ctx软件队列</h4><p>在重io的服务器场景下，request的数量是庞大的，为了减少全局锁的使用，避免内核同步带来的麻烦，kernel将blk_mq_ctx定义为per cpu变量，每个request仅可以加到本cpu的blk_mq_ctx链表上，这也是kernel惯用的策略。在另一方面，io请求分为读和写，在nvme设备中读和写请求共用一个queue时，写请求会将读请求阻塞，因此kernel总结出三种模式供request进行选择 default模式、只读模式、poll轮询模式（详见HCTX_MAX_TYPES的定义），ctx和hctx都遵循这个标准。</p>
<img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87-19.png" class="" title="图片描述">

<p>如果不使用plug机制（也就是不加blk_start_plug函数），此时若有一个只读request，它会直接根据自己的request-&gt;mq_ctx成员，找到自己所映射的是哪个磁盘的ctx，然后将request加到当前cpu的ctx.rq_lists[HCTX_TYPE_READ]链表中，关于plug链表中所做的bio合并操作在ctx.rq_lists链表中也会重新做一遍。如果使用plug机制，plug链表和ctx.rq_lists链表二者也并不冲突，blk_plug.mq_list最终也会通过blk_finish_plug或者主动调用blk_flush_plug，重新将request加到ctx.rq_lists中。</p>
<p>代码参考：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bio合并：</span><br><span class="line">blk_mq_submit_bio -&gt; blk_mq_get_new_requests -&gt; blk_mq_sched_bio_merge -&gt; blk_bio_list_merge -&gt; blk_attempt_bio_merge</span><br><span class="line">request插入ctx：blk_mq_submit_bio -&gt; blk_mq_sched_insert_request -&gt; __ blk_mq_insert_request -&gt; __ blk_mq_insert_req_list -&gt; list_add(&amp;rq-&gt;queuelist, &amp;ctx-&gt;rq_lists[type])</span><br><span class="line"></span><br><span class="line">取出request：</span><br><span class="line">blk_mq_run_hw_queue -&gt; __ blk_mq_delay_run_hw_queue -&gt; __ blk_mq_run_hw_queue -&gt; blk_mq_sched_dispatch_requests -&gt; __ blk_mq_sched_dispatch_requests -&gt; blk_mq_do_dispatch_ctx -&gt; blk_mq_dequeue_from_ctx -&gt; dispatch_rq_from_ctx</span><br><span class="line">__blk_mq_sched_dispatch_requests -&gt; blk_mq_flush_busy_ctxs（取出）/blk_mq_dispatch_rq_list（发送给磁盘）</span><br></pre></td></tr></table></figure>

<h4 id="blk-mq-hw-ctx硬件队列"><a href="#blk-mq-hw-ctx硬件队列" class="headerlink" title="blk_mq_hw_ctx硬件队列"></a>blk_mq_hw_ctx硬件队列</h4><p>机械磁盘只有一个磁头在不断的旋转扫描扇区数据，所以它只有一个数据传输通道，但是对于flash而言，可能硬件上支持多个通道同时传输数据，所以磁盘驱动又定义了一种新的per disk per channel的数据结构struct blk_mq_hw_ctx。</p>
<p>首先我对已知的request队列管理模式做了个简单的总结</p>
<img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87-20.png" class="" title="图片描述">

<p>关于plug和ctx已经介绍过了，并且ctx和hctx都是per disk的结构体，但是per cpu的ctx如何映射per channel hctx呢？其实没有那么复杂，默认场景下它们仅仅是一一对应的关系，但是驱动也能根据blk_mq_ops-&gt;map_queues回调函数自定义映射关系。</p>
<p>这里以默认的HCTX_TYPE_DEFAULT模式举例：</p>
<img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87-21.png" class="" title="图片描述">

<p>HCTX_TYPE_READ和HCTX_TYPE_POLL（如果有）也是一样的映射关系，并且可以做到硬件通道层面的隔离，kernel也做了一个优化，当cpu数目大于硬件通道数目时，同一物理cpu的不同虚拟cpu所对应的ctx，会映射到同一hctx。</p>
<p>代码参考：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">blk_mq_alloc_tag_set-&gt;blk_mq_update_queue_map-&gt;blk_mq_map_queues</span><br></pre></td></tr></table></figure>

<p>内核支持同步和异步两种方式发送request，在hctx中维护了一个delayed_work，用于异步方式往disk发送request，避免进程由于磁盘性能问题阻塞。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">blk_mq_sched_insert_request -&gt; blk_mq_run_hw_queue -&gt; __ blk_mq_delay_run_hw_queue -&gt;（异步分支：hctx.run_work -&gt; blk_mq_run_work_fn） -&gt; __ blk_mq_run_hw_queue -&gt; blk_mq_sched_dispatch_requests -&gt; __ blk_mq_sched_dispatch_requests -&gt; blk_mq_dispatch_rq_list -&gt; .queue_rq</span><br></pre></td></tr></table></figure>

<p>request经过合并和排序之后，不断的调用磁盘驱动的回调函数.queue_rq，将request发送给磁盘。磁盘驱动会根据request和hctx记录的硬件信息生成总线对应的通信指令（如果磁盘是scsi设备，则会生成scsi_cmd），存储到struct request结尾（参考函数blk_mq_rq_to_pdu）。仔细阅读代码会发现每次申请struct request结构体时，会申请远大于struct request结构体本身大小的内存，多出来的部分通过磁盘驱动.init_request回调函数填充cmd（参考blk_mq_alloc_rqs），直接用于通信。</p>
<p>代码再深入便是硬件厂商维护的驱动代码了，对于这种五花八门的驱动代码，确实提不起兴致，大家有需求自己琢磨吧。</p>
<h2 id="涉及的数据大小"><a href="#涉及的数据大小" class="headerlink" title="涉及的数据大小"></a>涉及的数据大小</h2><h3 id="Sector-扇区-："><a href="#Sector-扇区-：" class="headerlink" title="Sector(扇区)："></a>Sector(扇区)：</h3><p>发块设备读写的最小单位。看块设备驱动可以知道，这是软件概念，和硬件没有关系直接关系。虽说没有直接关系，但这个大小设置关系到软件处理的复杂度。例如有些存储设备一次性读写的真是的大小是1024B，那该存储设备的驱动，就要将每次收到的sector个数再处理为一次1024B的数据请求。<br>在Linux内核中，需要调用<code>blk_queue_logical_block_size</code>接口设置block size。<br>其大小保存在<code>struct request_queue.limits.logical_block_size</code>中。</p>
<h3 id="Block-块-："><a href="#Block-块-：" class="headerlink" title="Block(块)："></a>Block(块)：</h3><p>VFS和文件系统处理数据的最小单位。Block大小必须是2的幂大小，大小不能大于一个page且必须是Sector大小的整数倍。<br>每个Block都需要内存来保存读写的数据，这个内存由buffer_head管理。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">buffer_head</span> &#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">page</span> *<span class="title">b_page</span>;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">block_device</span> *<span class="title">b_bdev</span>;</span></span><br><span class="line">    <span class="type">sector_t</span> b_blocknr;</span><br><span class="line">    <span class="type">char</span> *b_data;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>当前io子系统中、<code>struct buffer_head</code>主要是用来读写文件系统的<code>inode</code>之类的信息数据、文件的内容一般使用<code>struct page</code>和<code>struct bio</code>等直接读写。<br><code>struct buffer_head</code>文件系统的结构体成员意义如下：<br>b_page：表示申请的page（每个Block大小不能大于一个page大小，所以这里就最多保存一个page了）。<br>b_bdev：指向Block处理此Block数据的块设备。<br>b_blocknr：表示在分区中，Block的index（在整个分区中的第几个）。<br>b_data：b_page的地址。</p>
<p>Block大小在文件系统初始化的时候，由mkfs等格式化的时候指定。（如mkfs.ext4 -b 4096）。<br>Block大小会给文件系统性能带来直接的性能影响。根据经验、如果你的文件系统要处理很多小文件、那Block大小小一点对性能会好。反之Block大小大一点对性能好。</p>
<h3 id="Segment"><a href="#Segment" class="headerlink" title="Segment"></a>Segment</h3><p>Segment是单个I&#x2F;O请求中连续物理内存区域的描述单元，用于支持Scatter-Gather（分散-聚集）DMA传输。Segment是块设备驱动层的概念，与硬件DMA能里相关。<br>每个Segment中可以包含单个或多个Block，每个Block又可以包含多个Sector。但有个约束就是每个Segment中包含的这些Sector都应该是在物理设备上连续的。<br>Segment在Linux内核中使用<code>struct bio_vec</code>来表示：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">bio_vec</span> &#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">page</span> *<span class="title">bv_page</span>;</span>     </span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> bv_len;      </span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> bv_offset;   </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>每次request发送多个Segment，使用<code>struct bio</code>来表示。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">bio</span> &#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">bio_vec</span> *<span class="title">bi_io_vec</span>;</span> </span><br><span class="line">    <span class="type">unsigned</span> <span class="type">short</span> bi_vcnt;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>在读写的过程中，数据流如下：</p>
<ol>
<li>文件系统层：文件操作将数据按Block（如1KB）划分，提交IO请求到块设备</li>
<li>块设备层：将Block划分为Sector序列（1KB-&gt;512B），组织Sector成Segment，提交DMA拷贝（大部份如此）</li>
<li>硬件层：按照Sector大小读写数据，并使用Segment传输数据。</li>
</ol>
<p>一个page内disk数据布局如下：</p>
<img src="/blog/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/%E5%9D%97%E8%AE%BE%E5%A4%87-02.png" class="" title="图片描述">

<h2 id="io调度"><a href="#io调度" class="headerlink" title="io调度"></a>io调度</h2><p>io调度的目的、主要就是为了减少实际访问硬件的频次、例如将多个相邻的写合并成一个等。<br>这节主要看一下io调度器的种类与特点和用途。应该是要分单队列的和多队列的（blk-mq）看的、但单队列的不看了、现在都很少看。所以下面将的都是多队列的。</p>
<h3 id="调度算法介绍"><a href="#调度算法介绍" class="headerlink" title="调度算法介绍"></a>调度算法介绍</h3><p>1.无调度</p>
<ul>
<li>针对本身有很强调度能力的设备（调度就交给硬件、软件不进行额外的调度）</li>
<li>主要用于NVMe SSD等（设备自身的调度能力很强）</li>
<li>涉及的源码：<code>block/blk-mq-sched.c</code>（核心框架）、<code>block/blk-mq.c</code>（请求分发）</li>
</ul>
<p>2.Kyber</p>
<ul>
<li>主要针对访问延时较低的设备</li>
<li>主要用于SSD等</li>
<li>算法特点：(1)分离读写队列 (2) 基于令牌桶的深度限制 (3) 自适应调度策略</li>
<li>涉及的源码：<code>block/kyber-iosched.c</code>（核心框架）、<code>block/blk-stat.c</code>（io统计）</li>
</ul>
<p>3.BFQ(Budget Fair Queuing)</p>
<ul>
<li>为了公平分配io带宽、优化交互式应用响应设计</li>
<li>算法特点：(1) 基于时间片分配io预算 (2) 支持cgroup权重控制 (3) 低延迟保障</li>
<li>涉及的源码：<code>block/bfq-iosched.c</code>（核心框架）、<code>block/bfq-wf2q.c</code>（加权公平队列算法）</li>
</ul>
<p>4.MQ-Deadline</p>
<ul>
<li>为了防止饿死、确保请求截止时间</li>
<li>算法特点：(1) 读写请求分离 (2) 截止时间检查 </li>
<li>涉及的源码：<code>block/mq-deadline.c</code>（核心框架）、<code>block/elevator.c</code>（调度器注册）</li>
</ul>
<h3 id="io调度器选择"><a href="#io调度器选择" class="headerlink" title="io调度器选择"></a>io调度器选择</h3><table>
<thead>
<tr>
<th><strong>设备类型</strong></th>
<th><strong>推荐调度器</strong></th>
<th><strong>原因</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>NVMe SSD</strong></td>
<td>None 或 Kyber</td>
<td>避免软件调度开销，发挥硬件并行性</td>
</tr>
<tr>
<td><strong>SATA SSD</strong></td>
<td>MQ-Deadline</td>
<td>平衡延迟与吞吐，防止写请求饿死读请求</td>
</tr>
<tr>
<td><strong>HDD (机械硬盘)</strong></td>
<td>BFQ</td>
<td>公平分配带宽，优化交互响应（尤其多任务场景）</td>
</tr>
<tr>
<td><strong>虚拟机虚拟磁盘</strong></td>
<td>None</td>
<td>后端存储（如宿主机）已处理调度，Guest 无需额外开销</td>
</tr>
</tbody></table>
<h3 id="io调度器性能指标"><a href="#io调度器性能指标" class="headerlink" title="io调度器性能指标"></a>io调度器性能指标</h3><table>
<thead>
<tr>
<th><strong>指标</strong></th>
<th><strong>None</strong></th>
<th><strong>Kyber</strong></th>
<th><strong>BFQ</strong></th>
<th><strong>MQ-Deadline</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>吞吐量</strong></td>
<td>★★★★★</td>
<td>★★★★☆</td>
<td>★★★☆☆</td>
<td>★★★★☆</td>
</tr>
<tr>
<td><strong>延迟稳定性</strong></td>
<td>★★☆☆☆</td>
<td>★★★★☆</td>
<td>★★★★★</td>
<td>★★★★☆</td>
</tr>
<tr>
<td><strong>公平性</strong></td>
<td>☆☆☆☆☆</td>
<td>★★☆☆☆</td>
<td>★★★★★</td>
<td>★★★☆☆</td>
</tr>
<tr>
<td><strong>CPU 开销</strong></td>
<td>最低</td>
<td>低</td>
<td>高</td>
<td>中等</td>
</tr>
</tbody></table>
<p>其实ctx软件队列并不是必要的，kernel有很多可选的io调度器(elevator_queue)，例如：bpf、kyber、deadline，实现思想和cpu调度器类似。io调度器通过自定义的回调函数ops.insert_requests拿到新的request，使用一系列调度算法将request进行合并与重新排列，通过回调函数ops.dispatch_request输出最合适的request，跳过ctx机制，直接将request发送给磁盘。我认为ctx和elevator_queue两者是互相替代的关系，默认情况下使用ctx机制，在复杂的场景下可选择elevator_queue机制，举个例子，对于机械硬盘而言，磁头需要通过不停的旋转扫描盘片数据，我们总是希望多个request之间尽管不是连续的，但也尽量是一个顺序分布的关系，以减少磁盘旋转的范围，提高访问速度，在这种情况下，不同的io调度器提供的不同调度策略往往能起到更好的效果。 ctx和elevator_queue调度器本质上都是为了更高效的访问磁盘，殊途同归。具体io调度器的实现细节我也没看过，elevator_queue内部是否会用到ctx也不是很清楚，以后再来填这个坑吧。</p>
<p>代码参考：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">插入：</span><br><span class="line">    blk_mq_sched_insert_request -&gt; ops.insert_requests</span><br><span class="line">取出：</span><br><span class="line">    __ blk_mq_sched_dispatch_requests -&gt; blk_mq_do_dispatch_sched -&gt; __ blk_mq_do_dispatch_sched -&gt; ops.dispatch_request</span><br><span class="line">将request发送给disk：__ blk_mq_do_dispatch_sched -&gt; blk_mq_dispatch_rq_list -&gt; .queue_rq</span><br></pre></td></tr></table></figure>

<h2 id="块设备分区"><a href="#块设备分区" class="headerlink" title="块设备分区"></a>块设备分区</h2><p>使用<code>lsblk</code>来查看设备上有哪些块设备驱动：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS</span><br><span class="line">nvme0n1     259:0    0  1.9T  0 disk </span><br><span class="line">├─nvme0n1p1 259:1    0  260M  0 part </span><br><span class="line">├─nvme0n1p2 259:2    0   56G  0 part [SWAP]</span><br><span class="line">└─nvme0n1p3 259:3    0  1.8T  0 part /var/lib/docker/btrfs</span><br><span class="line">                                     /var</span><br><span class="line">                                     /tmp</span><br><span class="line">                                     /opt</span><br><span class="line">                                     /home</span><br><span class="line">                                     /</span><br></pre></td></tr></table></figure>

<p>可以看到系统上有一个<code>nvme0n1</code>的块设备且块设备被分区分成几个、如下方法再仔细查看细节：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ sudo fdisk -l /dev/nvme0n1</span><br><span class="line">Disk /dev/nvme0n1: 1.86 TiB, 2048408248320 bytes, 4000797360 sectors</span><br><span class="line">Disk model: INTEL SSDPEKNU020TZ                     </span><br><span class="line">Units: sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disklabel type: gpt</span><br><span class="line">Disk identifier: BAF03C52-854E-41AD-9C42-6DD5C0E9F156</span><br><span class="line"></span><br><span class="line">Device             Start        End    Sectors  Size Type</span><br><span class="line">/dev/nvme0n1p1      2048     534527     532480  260M EFI System</span><br><span class="line">/dev/nvme0n1p2    534528  117975039  117440512   56G Linux swap</span><br><span class="line">/dev/nvme0n1p3 117975040 4000796671 3882821632  1.8T Linux filesystem</span><br></pre></td></tr></table></figure>

<p>一个块设备上可以有一个或多个分区，而分区表则记录了分区的信息，常见的分区表有<code>MBR</code>和<code>GPT</code>两种格式。从上述打印可以看到该块设备使用了<code>GPT</code>分区表、有三个分区。<br>每个分区需要被格式化成某种文件系统后、才能存储和读写文件。</p>
<h2 id="块设备命名"><a href="#块设备命名" class="headerlink" title="块设备命名"></a>块设备命名</h2><p>块设备名的前缀指定了操作块设备时使用的驱动子系统。</p>
<h3 id="SCSI"><a href="#SCSI" class="headerlink" title="SCSI"></a>SCSI</h3><p>支持SCSI命令（SCSI、SAS、UASP）、ATA（PATA、SATA）的存储设备和USB大容量存储设备，比如固态硬盘、SATA接口的机械硬盘和U盘，均由SCSI驱动子系统处理。<br>这些设备的命名以sd开头，之后是一个从a开始的小写字母，a表示第一个发现的设备，b表示第二个发现的设备，以此类推。</p>
<p>例如：</p>
<ul>
<li><code>/dev/sda</code> - 设备 a，第一个发现的设备。</li>
<li><code>/dev/sde</code> - 设备 e，第五个发现的设备。</li>
</ul>
<h3 id="NVMe"><a href="#NVMe" class="headerlink" title="NVMe"></a>NVMe</h3><p>使用NVMe 协议的设备、比如NVMe固态硬盘，名称以nvme开头。之后是一个从0开始的数字，表示设备控制器（Controller）号。 nvme0 表示第一个发现的 NVMe 控制器，nvme1 表示第二个发现的，以此类推。<br>然后是字母n和一个从1开始的数字，表示控制器上的命名空间（Namespace）。 比如 nvme0n1 表示第一个控制器上的第一个命名空间，nvme0n2 表示第一个控制器上的第二个命名空间，以此类推。</p>
<p>例如：</p>
<ul>
<li><code>/dev/nvme0n1</code> - 控制器 0 上的命名空间 1，第一个控制器上的第一个命名空间。</li>
<li><code>/dev/nvme2n5</code> - 控制器 2 上的命名空间 5，第三个控制器上的第五个命名空间。</li>
</ul>
<h3 id="MMC"><a href="#MMC" class="headerlink" title="MMC"></a>MMC</h3><p>SD卡、多媒体存储卡(MMC卡)和 eMMC 存储设备由mmc驱动处理。这些设备的名称以mmcblk开头，之后是一个从 0 开始的数字表示设备号。 比如 mmcblk0 表示第一个发现的设备，mmcblk1 表示第二个发现的设备，以此类推。</p>
<p>例如：</p>
<ul>
<li><code>/dev/mmcblk0</code> - 设备 0，第一个发现的设备。</li>
<li><code>/dev/mmcblk4</code> - 设备 4，第五个发现的设备。</li>
</ul>
<h3 id="SCSI-光盘驱动器"><a href="#SCSI-光盘驱动器" class="headerlink" title="SCSI 光盘驱动器"></a>SCSI 光盘驱动器</h3><p>通过 SCSI 驱动子系统支持的接口连接的光盘驱动器，其名称以 sr 开头。之后是一个从 0 开始的数字，表示设备号。比如 sr0 表示第一个发现的设备，sr1 表示第二个发现的设备，以此类推。 udev 也提供到 &#x2F;dev&#x2F;sr0 的软链接，名为 &#x2F;dev&#x2F;cdrom。软链接的名称 &#x2F;dev&#x2F;cdrom 与驱动器支持的光盘类型和插入的介质无关。</p>
<p>例如：</p>
<ul>
<li><code>/dev/sr0</code> - 光盘驱动器 0，第一个发现的光盘驱动器。</li>
<li><code>/dev/sr4</code> - 光盘驱动器 4，第五个发现的光盘驱动器。</li>
<li><code>/dev/cdrom</code> - 到 &#x2F;dev&#x2F;sr0 的符号链接。</li>
</ul>
<h3 id="VirtIO-块设备"><a href="#VirtIO-块设备" class="headerlink" title="VirtIO 块设备"></a>VirtIO 块设备</h3><p>VirtIO块设备的名称以vd开头。之后是一个从a开始的小写字母，a表示第一个发现的设备(vda)，b 表示第二个发现的设备(vdb)，以此类推。</p>
<p>例如：</p>
<ul>
<li><code>/dev/vda</code> - 设备 a，第一个发现的设备。</li>
<li><code>/dev/vde</code> - 设备 e，第五个发现的设备。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://201.ustclug.org/ops/storage/filesystem/">https://201.ustclug.org/ops/storage/filesystem/</a></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://github.com/CodeImp/sblkdev">https://github.com/CodeImp/sblkdev</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bluepuni.com/archives/linux-blk-mq/">https://www.bluepuni.com/archives/linux-blk-mq/</a></p>
<p><a target="_blank" rel="noopener" href="https://qemu-project.gitlab.io/qemu/devel/vfio-iommufd.html">https://qemu-project.gitlab.io/qemu/devel/vfio-iommufd.html</a></p>
<p><a target="_blank" rel="noopener" href="https://aliez22.github.io/posts/11537/">https://aliez22.github.io/posts/11537/</a></p>
<p>&#x2F;&#x2F; 日志文件系统<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/107558961">https://zhuanlan.zhihu.com/p/107558961</a></p>
<p>chrome-extension:&#x2F;&#x2F;efaidnbmnnnibpcajpcglclefindmkaj&#x2F;<a target="_blank" rel="noopener" href="https://lrita.github.io/images/posts/filesystem/Linux.Kernel.Read.Procedure.pdf">https://lrita.github.io/images/posts/filesystem/Linux.Kernel.Read.Procedure.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/home19900111/article/details/126034992">https://blog.csdn.net/home19900111/article/details/126034992</a></p>
<p><a target="_blank" rel="noopener" href="https://goldeneye2.videolan.org/OctopusET/linux/-/tree/v5.12/fs/iomap?ref_type=tags">https://goldeneye2.videolan.org/OctopusET/linux/-/tree/v5.12/fs/iomap?ref_type=tags</a></p>
<p>&#x2F;&#x2F; 日志型文件系统<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/107558961">https://zhuanlan.zhihu.com/p/107558961</a></p>
<p>&#x2F;&#x2F; buffer_head<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/bin_linux96/article/details/111315531">https://blog.csdn.net/bin_linux96/article/details/111315531</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.kernel.org/filesystems/iomap/design.html">https://docs.kernel.org/filesystems/iomap/design.html</a></p>
<p>&#x2F;&#x2F; bio<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/545906763">https://zhuanlan.zhihu.com/p/545906763</a></p>
<p>&#x2F;&#x2F; 这个很好<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/545906763">https://zhuanlan.zhihu.com/p/545906763</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/sctb/p/14022008.html">https://www.cnblogs.com/sctb/p/14022008.html</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/606666107">https://zhuanlan.zhihu.com/p/606666107</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://ray-jk-hong.github.io">ray.jk.hong</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://ray-jk-hong.github.io/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/">https://ray-jk-hong.github.io/2025/08/13/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E8%AE%BE%E5%A4%87%E5%9F%BA%E7%A1%80/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://ray-jk-hong.github.io" target="_blank">ray.jk.hong</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/blog/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/">文件系统</a></div><div class="post-share"><div class="social-share" data-image="/blog/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/blog/2025/08/13/ai/ai%E7%AE%97%E5%8A%9B/" title="AI算力"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">AI算力</div></div><div class="info-2"><div class="info-item-1">FLOPSFLOPS是floating point operations per second的缩写（注意是全部大写）、意思是”每秒浮点运算次数”。它常被用来估算电脑的执行效能，尤其是在使用到大量浮点运算的科学计算领域中。  MFLOPS(Mega FLOPS)：等于每秒一百万10^6次的浮点运算 GFLOPS(Giga FLOPS)：等于每秒十亿=10^9次浮点运算 BFLOPS（Billion FLOPS）：等于每秒十亿=10^9次浮点运算、与GFLOPS是一样的 TFLOPS（teraFLOPS）：等于每秒一万亿=10^12次浮点运算  PFLOPS（petaFLOPS）：等于每秒一千万亿=10^15次浮点运算 EFLOPS（exaFLOPS）：等于每秒一佰京=10^18次浮点运算  FLOPsFLOPs是floating point operations的缩写（注意最后一个是小写）、是”浮点运算次数”。它常备用来衡量算法和模型的复杂度。 FPFP是floating...</div></div></div></a><a class="pagination-related" href="/blog/2025/08/14/arch/Arm-mmu/" title="Arm-mmu"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Arm-mmu</div></div><div class="info-2"><div class="info-item-1">MMU负责将虚拟地址转成物理地址。除了地址转换，还可以控制一下几个事情：  内存访问权限控制 内存访问ordering控制 Cache访问策略下面就按照几个功能的来说明，并顺带说明没个功能所涉及的寄存器。  启动阶段配置在完成地址翻译的时候，在启动阶段要确认并配置以下几个寄存器。 TCR_EL寄存器  上图是TCR_EL寄存器的显示图。TCR寄存器的设置在arch/arm64/mm/proc.S的__cpu_setup函数中。 12345mov_q	tcr, TCR_TxSZ(VA_BITS) | TCR_CACHE_FLAGS | TCR_SMP_FLAGS | \		TCR_TG_FLAGS | TCR_KASLR_FLAGS | TCR_ASID16 | \		TCR_TBI0 | TCR_A1 | TCR_KASAN_SW_FLAGS | TCR_MTE_FLAGS   ...   tcr_compute_pa_size tcr, #TCR_IPS_SHIFT, x5,...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/blog/2025/08/19/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/PageCache/" title="Page Cache"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-19</div><div class="info-item-2">Page Cache</div></div><div class="info-2"><div class="info-item-1">介绍Linux内核为了文件系统的读写性能、提供了page cache功能进行读写数据的缓存。这一功能存利用一部分系统物理内存，确保最重要、最常使用的块设备数据在操作时可直接从主内存获取，而无须从低速设备读取。物理内存还用于存储从块设备读取的数据，使得随后对该数据的访问可直接在物理内存进行，而无须从外部设备再次取用。Linux内核为块设备提供了两种缓存方案：(1) 页缓存（ page cache）针对以页为单位的所有操作，并考虑了特定体系结构上的页长度。一个主要的例子是许多章讨论过的内存映射技术。因为其他类型的文件访问也是基于内核中的这一技术实现的，所以页缓存实际上负责了块设备的大部分缓存工作。(2) 块缓存（ buffer...</div></div></div></a><a class="pagination-related" href="/blog/2025/09/22/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/ext4-%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/" title="ext4日志系统"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-22</div><div class="info-item-2">ext4日志系统</div></div><div class="info-2"><div class="info-item-1">https://blog.51cto.com/u_15181572/6173476 https://blog.csdn.net/SweeNeil/article/details/87935992 </div></div></div></a><a class="pagination-related" href="/blog/2025/09/22/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/ext4/" title="ext4文件系统"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-22</div><div class="info-item-2">ext4文件系统</div></div><div class="info-2"><div class="info-item-1">创建挂载文件系统创建文件按如下命令创建1GB的文件： 1dd if=/dev/zero of=./ext4_image.img bs=1M count=1024  格式化按如下命令将文件格式化成ext4文件系统： 1mkfs.ext4 ext4_image.img  挂载文件系统通过Linux的loop虚拟设备将文件挂载到目录上： 1sudo mount -o loop ext4_image.img /home/biao/test/ext4/ext4_simulator  查看文件系统信息按照如下命令、将挂载之后的文件系统文件信息打印出来： 1dumpe2fs ext4_image.img  执行之后内容如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576dumpe2fs 1.44.1 (24-Mar-2018)Filesystem...</div></div></div></a><a class="pagination-related" href="/blog/2025/09/22/linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/shmem/" title="shmem"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-22</div><div class="info-item-2">shmem</div></div><div class="info-2"><div class="info-item-1">介绍页面类型通常Linux系统中，主要的页面类型如下：   主要的类型有Page Cache类型和Swap Cache类型。而shmem页面既有匿名页的特点（page-&gt;flags设置PG_swapbacked , 具有swap功能），也有文件页的特点（inode-&gt;i_mapping-&gt;a_ops &#x3D; &amp;shmem_aops, 关联文件，有page cache）。（在哪里设置了这个?） LRU类型Linux内核中匿名页通过Swap方式、文件页通过Page Cache方式缓存。这些页会被加入到对应的LRU链表中进行老化回收等操作。LRU的链表类型定义如下： 12345678enum lru_list &#123;	LRU_INACTIVE_ANON = LRU_BASE,	LRU_ACTIVE_ANON = LRU_BASE + LRU_ACTIVE,	LRU_INACTIVE_FILE = LRU_BASE + LRU_FILE,	LRU_ACTIVE_FILE = LRU_BASE + LRU_FILE +...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/blog/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/blog/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">ray.jk.hong</div><div class="author-info-description"></div><div class="site-data"><a href="/blog/archives/"><div class="headline">文章</div><div class="length-num">52</div></a><a href="/blog/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/blog/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ray-jk-hong"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B"><span class="toc-number">1.</span> <span class="toc-text">数据处理流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9D%97%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8"><span class="toc-number">2.</span> <span class="toc-text">块设备驱动</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%97%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.1.</span> <span class="toc-text">块设备驱动介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%97%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8-1"><span class="toc-number">2.2.</span> <span class="toc-text">块设备驱动</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%A8%E5%86%8C%E5%9D%97%E8%AE%BE%E5%A4%87%E4%B8%BB%E8%AE%BE%E5%A4%87%E5%8F%B7"><span class="toc-number">2.2.1.</span> <span class="toc-text">注册块设备主设备号</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAgendisk"><span class="toc-number">2.2.2.</span> <span class="toc-text">创建gendisk</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9D%97%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8%E6%95%B0%E6%8D%AE%E6%B5%81"><span class="toc-number">2.2.3.</span> <span class="toc-text">块设备驱动数据流</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9D%97%E8%AE%BE%E5%A4%87"><span class="toc-number">3.</span> <span class="toc-text">块设备</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#struct-kiocb-struct-iov-iter"><span class="toc-number">3.1.</span> <span class="toc-text">struct kiocb&#x2F;struct iov_iter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#struct-bio-struct-bio-vec"><span class="toc-number">3.2.</span> <span class="toc-text">struct bio&#x2F;struct bio_vec</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#iomap"><span class="toc-number">3.3.</span> <span class="toc-text">iomap</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#request"><span class="toc-number">3.4.</span> <span class="toc-text">request</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#current-plug%E9%93%BE%E8%A1%A8"><span class="toc-number">3.5.</span> <span class="toc-text">current-&gt;plug链表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multi-queue%E5%A4%9A%E9%98%9F%E5%88%97%E6%8E%92%E9%98%9F%E6%9C%BA%E5%88%B6"><span class="toc-number">3.6.</span> <span class="toc-text">multi-queue多队列排队机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#blk-mq-ctx%E8%BD%AF%E4%BB%B6%E9%98%9F%E5%88%97"><span class="toc-number">3.6.1.</span> <span class="toc-text">blk_mq_ctx软件队列</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#blk-mq-hw-ctx%E7%A1%AC%E4%BB%B6%E9%98%9F%E5%88%97"><span class="toc-number">3.6.2.</span> <span class="toc-text">blk_mq_hw_ctx硬件队列</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B6%89%E5%8F%8A%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A4%A7%E5%B0%8F"><span class="toc-number">4.</span> <span class="toc-text">涉及的数据大小</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Sector-%E6%89%87%E5%8C%BA-%EF%BC%9A"><span class="toc-number">4.1.</span> <span class="toc-text">Sector(扇区)：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Block-%E5%9D%97-%EF%BC%9A"><span class="toc-number">4.2.</span> <span class="toc-text">Block(块)：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Segment"><span class="toc-number">4.3.</span> <span class="toc-text">Segment</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#io%E8%B0%83%E5%BA%A6"><span class="toc-number">5.</span> <span class="toc-text">io调度</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D"><span class="toc-number">5.1.</span> <span class="toc-text">调度算法介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#io%E8%B0%83%E5%BA%A6%E5%99%A8%E9%80%89%E6%8B%A9"><span class="toc-number">5.2.</span> <span class="toc-text">io调度器选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#io%E8%B0%83%E5%BA%A6%E5%99%A8%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87"><span class="toc-number">5.3.</span> <span class="toc-text">io调度器性能指标</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9D%97%E8%AE%BE%E5%A4%87%E5%88%86%E5%8C%BA"><span class="toc-number">6.</span> <span class="toc-text">块设备分区</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9D%97%E8%AE%BE%E5%A4%87%E5%91%BD%E5%90%8D"><span class="toc-number">7.</span> <span class="toc-text">块设备命名</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SCSI"><span class="toc-number">7.1.</span> <span class="toc-text">SCSI</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NVMe"><span class="toc-number">7.2.</span> <span class="toc-text">NVMe</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MMC"><span class="toc-number">7.3.</span> <span class="toc-text">MMC</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SCSI-%E5%85%89%E7%9B%98%E9%A9%B1%E5%8A%A8%E5%99%A8"><span class="toc-number">7.4.</span> <span class="toc-text">SCSI 光盘驱动器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#VirtIO-%E5%9D%97%E8%AE%BE%E5%A4%87"><span class="toc-number">7.5.</span> <span class="toc-text">VirtIO 块设备</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">8.</span> <span class="toc-text">参考</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/blog/js/utils.js"></script><script src="/blog/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>